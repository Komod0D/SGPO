# Training configuration
training:
  device: 0
  seed: 42
  epochs: 40
  batch_size: 16
  num_workers: 0
  verbose: false

# Model configuration
model:
  SSL_emb_dim: 256
  protein_backbone_model: "ProtT5"  # choices: ["ProtT5", "ESM2"]
  text_backbone_model: "SciBERT"
  reaction_backbone_model: "rxnfp"

# Sequence length limits
sequence_lengths:
  protein_max_sequence_len: 512
  text_max_sequence_len: 512
  reaction_max_sequence_len: 512

# Learning rates
learning_rates:
  protein_lr: 1e-5
  protein_lr_scale: 1.0
  text_lr: 1e-5
  text_lr_scale: 0.1
  reaction_lr: 1e-5
  reaction_lr_scale: 1.0
  decay: 0.0

# Contrastive learning
contrastive_learning:
  CL_neg_samples: 1
  CL_loss: "EBM_NCE"  # choices: ["EBM_NCE", "InfoNCE"]
  T: 0.1
  normalize: false

# Training setup
training_setup:
  use_three_modalities: true
  alpha_contrastive: 1.0
  alpha_generative: 0.0  # don't use this loss for now

# Data paths
data:
  dataset_path: "../../processed_data/"
  train_split: "easy_reaction_train"  # choices: ["easy_reaction_train", "medium_reaction_train", "hard_reaction_train", "reaction2EC"]

# Output
output:
  output_model_dir: null